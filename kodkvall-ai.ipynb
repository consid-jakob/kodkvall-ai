{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ddec29",
   "metadata": {},
   "source": [
    "## Kodkväll-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf4a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#hyperparams\n",
    "\n",
    "gamma = .99\n",
    "batch_size = 32\n",
    "buffer_size = 50000\n",
    "min_replay_size = 1000\n",
    "eps_start = 1.0\n",
    "eps_end = 0.02\n",
    "eps_decay_steps = 10000\n",
    "target_update_freq = 1000\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c938a25",
   "metadata": {},
   "source": [
    "### Miljö\n",
    "\n",
    "Simulatorn kommer från Open AI och går att läsa mer om här: [Open AIs simulator](https://gym.openai.com/envs/CartPole-v0)\n",
    "\n",
    "*En pole är fäst på en släde, som rör sig längsmed en friktions fri bana. Man kan påvreka polen genom att ge den en kraft av +1 eller -1. Simulereingen börjar med att polen står rakt upp och målet är att den inte ska falla. För verje tidssteg som pole klarar sig ges +1 poäng. Episoden avslutas om polen avviker mer än 2.4 enheter från center eller om dess vinkel är mer än 15 grader från en vertikal position. Episoden avslutat automatisk efter 200 steg.*\n",
    "\n",
    "\n",
    "![title](./cartpole.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Render env\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774ab48",
   "metadata": {},
   "source": [
    "### Utforskning\n",
    "\n",
    "för att agenten ska kuna utforska men även utnyttja tidigar lärdommar behövs en algorithm som tar hänsyn till båda dess aspekter. Här behöver vi implementera Epsilon-greedy algorithmen som tillåter detta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(eps):\n",
    "    '''\n",
    "    Beräknar en action enligt epsilon-greedy policyn.\n",
    "    Inputs:\n",
    "        eps    - utforskningsparameter.\n",
    "    Returns:\n",
    "        action     - Vald action (0 eller 1, med andra ord, vänster eller höger), Integer.\n",
    "    '''\n",
    "    #Code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dccbe",
   "metadata": {},
   "source": [
    "### Neuralt nätverk för Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    '''\n",
    "    Neuralt nätverk bestående av\n",
    "    Inputs:\n",
    "        eps    - utforskningsparameter.\n",
    "    Returns:\n",
    "        action     - Vald action (0 eller 1, med andra ord, vänster eller höger), Integer.\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Create neural net structure here and bind to self.net\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51bb72",
   "metadata": {},
   "source": [
    "### Initialisering av miljö och buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "rew_buffer = deque([0,0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=learning_rate)\n",
    "\n",
    "#Init buffer\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(min_replay_size):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    trans = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(trans)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae43c46",
   "metadata": {},
   "source": [
    "### Träning av agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdac68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    eps = np.interp(step, [0, eps_decay_steps], [eps_start, eps_end])\n",
    "\n",
    "    action = epsilon_greedy(eps)\n",
    "    \n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    trans = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(trans)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "        rew_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "    #after solved, show env\n",
    "    if len(rew_buffer) >= 100:\n",
    "        if np.mean(rew_buffer) >= 195:\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                obs, _, done, _ = env.step(action)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "\n",
    "    #gradient step\n",
    "    transitions = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "    obses = np.asanyarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asanyarray([t[4] for t in transitions])\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    #compute targets\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdims=True)[0]\n",
    "\n",
    "    targets = rews_t + gamma * (1 - dones_t) * max_target_q_values\n",
    "    \n",
    "    #compute loss\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(q_values, dim=1, index=actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "    #gradient decent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #update target net\n",
    "    if step % target_update_freq == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    #logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print('step: {}'.format(step))\n",
    "        print('average reward: {}'.format(np.mean(rew_buffer)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
